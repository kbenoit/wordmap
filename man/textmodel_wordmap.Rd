% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/textmodel.R
\name{textmodel_wordmap}
\alias{textmodel_wordmap}
\title{Semi-supervised Bayesian model for multinomial document classification}
\usage{
textmodel_wordmap(
  x,
  y,
  label = c("all", "max"),
  smooth = 1,
  boolean = FALSE,
  drop_label = TRUE,
  verbose = quanteda_options("verbose"),
  entropy = c("none", "global", "local", "average"),
  ...
)
}
\arguments{
\item{x}{a dfm or fcm created by \code{\link[quanteda:dfm]{quanteda::dfm()}}}

\item{y}{a dfm or a sparse matrix that record class membership of the
documents. It can be created applying \code{\link[quanteda:dfm_lookup]{quanteda::dfm_lookup()}} to \code{x}.}

\item{label}{if "max", uses only labels for the maximum value in each row of
\code{y}.}

\item{smooth}{a value added to the frequency of words to smooth likelihood
ratios.}

\item{boolean}{if \code{TRUE}, only consider presence or absence of features in
each document to limit the impact of words repeated in few documents.}

\item{drop_label}{if \code{TRUE}, drops empty columns of \code{y} and ignore their
labels.}

\item{verbose}{if \code{TRUE}, shows progress of training.}

\item{entropy}{[experimental] the scheme to compute the entropy to
regularize likelihood ratios. The entropy of features are computed over
labels if \code{global} or over documents with the same labels if \code{local}. Local
entropy is averaged if \code{average}. See the details.}

\item{...}{additional arguments passed to internal functions.}
}
\description{
Train a Wordmap model using labels given by a dictionary analysis.
}
\details{
Wordsmap learns association between words and classes as likelihood
ratios based on the features in \code{x} and the labels in \code{y}. The large
likelihood ratios tend to concentrate to a small number of features but the
entropy of their frequencies over labels or documents helps to disperse the
distribution.
}
\examples{
require(quanteda)

# split into sentences
corp <- corpus_reshape(data_corpus_ungd2017)

# tokenize
toks <- tokens(corp, remove_punct = TRUE) \%>\%
   tokens_remove(stopwords("en"))

# apply seed dictionary
toks_dict <- tokens_lookup(toks, data_dictionary_topic)

# form dfm
dfmt_feat <- dfm(toks)
dfmt_dict <- dfm(toks_dict)

# fit wordmap model
map <- textmodel_wordmap(dfmt_feat, dfmt_dict)
coef(map)
predict(map)

}
\references{
Kohei Watanabe. 2018. "\href{https://www.tandfonline.com/eprint/dDeyUTBrhxBSSkHPn5uB/full}{Newsmap: semi-supervised approach to geographical news classification.}"
\emph{Digital Journalism} 6(3): 294-309.
}
